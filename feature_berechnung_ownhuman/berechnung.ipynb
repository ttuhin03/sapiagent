{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)   # Show all columns\n",
    "pd.set_option('display.max_rows', None)      # Show all rows if needed (optional)\n",
    "pd.set_option('display.width', None)         # Let the notebook handle line wrapping\n",
    "pd.set_option('display.max_colwidth', None)  \n",
    "\n",
    "pfadDaten = \"/Users/tuhin/Desktop/Bachelorarbeit/sapiagent/sapimouse_ownhumandata/user4/session_2024_12_30_3min.csv\"\n",
    "\n",
    "df = pd.read_csv(pfadDaten)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# 1. Sort by timestamp if not sorted\n",
    "df = df.sort_values('client timestamp')\n",
    "\n",
    "# 2. Compute the time difference to the previous row\n",
    "df['time_diff'] = df['client timestamp'].diff().fillna(0)\n",
    "\n",
    "# 3. Define a \"new_chunk\" marker where conditions are met\n",
    "#    Condition A: time gap > 4000\n",
    "#    Condition B: state == 'Released'\n",
    "df['new_chunk'] = (\n",
    "    (df['time_diff'] > 4000)    # large gap\n",
    "    | (df['state'] == 'Released')  # or row is 'Released'\n",
    ")\n",
    "\n",
    "# 4. Convert that boolean into a cumulative sum\n",
    "#    Each True increments the chunk ID\n",
    "df['chunk_id'] = df['new_chunk'].cumsum()\n",
    "\n",
    "# 5. (Optional) If you prefer the row with Released to be \n",
    "#    included in the preceding chunk rather than marking \n",
    "#    the start of the new chunk, you can adjust the logic as needed. \n",
    "#    For example, you might shift the condition or handle it differently.\n",
    "#    But in this version, chunk_id changes on the same row that has \"Released\".\n",
    "\n",
    "# Now each chunk is all rows that have the same chunk_id:\n",
    "# For example, group by chunk_id:\n",
    "groups = df.groupby('chunk_id')\n",
    "\n",
    "for chunk_id, group_data in groups:\n",
    "    print(f\"Chunk ID: {chunk_id}\")\n",
    "    print(group_data)\n",
    "    print(\"-----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "for chunk_id, subset in groups:\n",
    "    fig = px.line(\n",
    "        subset,\n",
    "        x='x',\n",
    "        y='y',\n",
    "        markers=True,\n",
    "        title=f\"Chunk ID: {chunk_id}\"\n",
    "    )\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Berechnung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothness(values):\n",
    "    n = len(values)\n",
    "    total_diff = 0.0\n",
    "\n",
    "    if n < 2:\n",
    "        return 0.0\n",
    "\n",
    "    for i in range(1, n):\n",
    "        total_diff += abs(values[i] - values[i-1])\n",
    "    return total_diff / (n - 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "for chunk_id, group_data in groups:\n",
    "    # Calculate distance between consecutive points\n",
    "    group_data['distance'] = (\n",
    "        (group_data['x'].diff()**2 + group_data['y'].diff()**2) ** 0.5\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Calculate time_diff between consecutive points; assumed to be in group_data already\n",
    "    # group_data['time_diff'] = ...\n",
    "    \n",
    "    # Calculate per-row velocity to get min and max\n",
    "    # Avoid division by zero by replacing 0 with NaN or a small number if needed\n",
    "    # Here, we’ll just replace time_diff == 0 with NaN:\n",
    "    group_data.loc[group_data['time_diff'] == 0, 'time_diff'] = float('nan')\n",
    "    group_data['row_velocity'] = group_data['distance'] / group_data['time_diff']\n",
    "\n",
    "    # Replace NaN back to 0 for velocity, if desired\n",
    "    group_data['row_velocity'] = group_data['row_velocity'].fillna(0)\n",
    "\n",
    "    # Differenz der Geschwindigkeiten:\n",
    "    velocity_diff = group_data['row_velocity'].diff()  # v[i] - v[i-1]\n",
    "\n",
    "    # Beschleunigung: diff in px/s / time_diff (ms) => px/s^2\n",
    "    group_data['row_acceleration'] = (velocity_diff / group_data['time_diff']) * 1000000\n",
    "    group_data['row_acceleration'] = group_data['row_acceleration'].fillna(0)\n",
    "\n",
    "    # Chunk-level total time\n",
    "    total_time = group_data['time_diff'].sum(skipna=True)\n",
    "    # Chunk-level total distance\n",
    "    total_distance = group_data['distance'].sum()\n",
    "    # Chunk-level average velocity (distance / time)\n",
    "    velocity_chunk = total_distance / total_time if total_time > 0 else 0\n",
    "\n",
    "    # Filter out rows where row_velocity is 0\n",
    "    nonzero_velocities = group_data.loc[group_data['row_velocity'] != 0, 'row_velocity']\n",
    "\n",
    "    # Compute the min from the non-zero velocities\n",
    "    velocity_min = nonzero_velocities.min() if not nonzero_velocities.empty else 0\n",
    "    velocity_max = group_data['row_velocity'].max()\n",
    "    velocity_mean = group_data['row_velocity'].mean()\n",
    "    velocity_var = group_data['row_velocity'].var()\n",
    "\n",
    "    # Beschleunigung auf Chunk-Ebene\n",
    "    acc_min = group_data['row_acceleration'].min()\n",
    "    acc_max = group_data['row_acceleration'].max()\n",
    "    acc_mean = group_data['row_acceleration'].mean()\n",
    "    acc_var = group_data['row_acceleration'].var()\n",
    "\n",
    "\n",
    "\n",
    "    # Direkte Distanz zwischen erstem und letztem Punkt\n",
    "    first_x, first_y = group_data.iloc[0]['x'], group_data.iloc[0]['y']\n",
    "    last_x, last_y   = group_data.iloc[-1]['x'], group_data.iloc[-1]['y']\n",
    "    direct_distance = np.sqrt((last_x - first_x)**2 + (last_y - first_y)**2)\n",
    "\n",
    "    # Beispiel-Dauer (Summe aller time_diff)\n",
    "    duration = total_time  # kann nach Bedarf in andere Einheiten (Min / Std) umgerechnet werden\n",
    "\n",
    "    # Effizienz als direkte Distanz / Gesamtzeit\n",
    "    # (falls du eine andere Definition für „Effizienz“ brauchst, entsprechend anpassen)\n",
    "    efficiency = direct_distance / duration if duration > 0 else 0\n",
    "\n",
    "\n",
    "\n",
    "    # If you also want the absolute difference (|delta_x|):\n",
    "    group_data['abs_delta_x'] = group_data['x'].diff().abs()\n",
    "    smoothness_v1 = group_data['abs_delta_x'].mean()\n",
    "\n",
    "        \n",
    "    smoothness_v2 = smoothness(group_data['x'].values)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Werte in das rows-Dictionary übernehmen\n",
    "    rows.append({\n",
    "        'chunk_id': chunk_id,\n",
    "        'geschwindigkeit': velocity_chunk * 1000,\n",
    "        'geschwindigkeit_min': velocity_min * 1000,\n",
    "        'geschwindigkeit_max': velocity_max * 1000,\n",
    "        'geschwindigkeit_mean': velocity_mean * 1000,\n",
    "        'geschwindigkeit_var': velocity_var * 1000,\n",
    "        'dauer': duration,\n",
    "        'direkte_distanz': direct_distance,\n",
    "        'effizienz': efficiency,\n",
    "        'totale_distanz': total_distance,\n",
    "        'smoothness_v1': smoothness_v1,\n",
    "        'smoothness_v2': smoothness_v2,\n",
    "        'beschleunigung_min':  acc_min,\n",
    "        'beschleunigung_max':  acc_max,\n",
    "        'beschleunigung_mean': acc_mean,\n",
    "        'beschleunigung_var':  acc_var,\n",
    "\n",
    "    })\n",
    "\n",
    "# Finally, build your DataFrame from the rows\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 4) Z-SCORE UND MINMAX-SCALING FÜR JEDES FEATURE\n",
    "# ------------------------------------------------------\n",
    "# Legen Sie fest, welche Spalten Sie skalieren möchten\n",
    "columns_to_scale = [\n",
    "    'geschwindigkeit', 'geschwindigkeit_min', 'geschwindigkeit_max',\n",
    "    'geschwindigkeit_mean', 'geschwindigkeit_var',\n",
    "    'beschleunigung_min', 'beschleunigung_max',\n",
    "    'beschleunigung_mean', 'beschleunigung_var',\n",
    "    'dauer', 'direkte_distanz', 'effizienz', 'totale_distanz',\n",
    "    'smoothness_v1', 'smoothness_v2'\n",
    "]\n",
    "\n",
    "# 4a) Z-SCORE => (X - mean)/std\n",
    "for col in columns_to_scale:\n",
    "    mean_val = df[col].mean()\n",
    "    std_val  = df[col].std()\n",
    "    # Falls std_val=0 => Division durch 0 vermeiden\n",
    "    if std_val == 0:\n",
    "        df[col + '_zscore'] = 0\n",
    "    else:\n",
    "        df[col + '_zscore'] = (df[col] - mean_val) / std_val\n",
    "\n",
    "# 4b) MIN-MAX-SCALING => (X - min)/(max - min)\n",
    "for col in columns_to_scale:\n",
    "    min_val = df[col].min()\n",
    "    max_val = df[col].max()\n",
    "    if max_val == min_val:\n",
    "        df[col + '_minmax'] = 0  # oder 1\n",
    "    else:\n",
    "        df[col + '_minmax'] = (df[col] - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['is_anomaly'] = 0\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Prepare the Data\n",
    "# -------------------------------\n",
    "\n",
    "# Assume your original DataFrame is named df.\n",
    "# We drop 'chunk_id' (an identifier) and 'is_anomaly' (the label)\n",
    "df_features = df.drop(columns=['chunk_id', 'is_anomaly'], errors='ignore')\n",
    "\n",
    "# Impute missing values in all feature columns using the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_features_imputed = pd.DataFrame(imputer.fit_transform(df_features), \n",
    "                                   columns=df_features.columns)\n",
    "\n",
    "# Convert features to a NumPy array\n",
    "X = df_features_imputed.values\n",
    "\n",
    "# Get the labels (assuming 0 = normal, 1 = anomaly)\n",
    "y = df['is_anomaly'].values\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Train/Test Split (Preserve Row Indices)\n",
    "# -------------------------------\n",
    "# We also pass df.index to preserve original row numbers for test samples.\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X, y, df.index, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# For One-Class SVM training, use only the \"normal\" samples (assume normal = 0)\n",
    "X_train_normal = X_train[y_train == 0]\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Scaling (if needed)\n",
    "# -------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_normal_scaled = scaler.fit_transform(X_train_normal)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Train One-Class SVM\n",
    "# -------------------------------\n",
    "clf = OneClassSVM(kernel='rbf', nu=0.1, gamma='auto')\n",
    "clf.fit(X_train_normal_scaled)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Predict and Evaluate\n",
    "# -------------------------------\n",
    "y_test_pred = clf.predict(X_test_scaled)\n",
    "# One-Class SVM returns +1 for normal and -1 for anomalies.\n",
    "# Map +1 -> 0 (normal) and -1 -> 1 (anomaly)\n",
    "y_test_pred_mapped = np.where(y_test_pred == 1, 0, 1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred_mapped)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Identify indices for each category\n",
    "tn_idx = (y_test == 0) & (y_test_pred_mapped == 0)  # True Normal\n",
    "fp_idx = (y_test == 0) & (y_test_pred_mapped == 1)  # False Anomaly (Normal flagged as anomaly)\n",
    "fn_idx = (y_test == 1) & (y_test_pred_mapped == 0)  # False Normal (Anomaly flagged as normal)\n",
    "tp_idx = (y_test == 1) & (y_test_pred_mapped == 1)  # True Anomaly\n",
    "\n",
    "print(\"\\nCounts:\")\n",
    "print(\"True Normal (TN):\", np.sum(tn_idx))\n",
    "print(\"False Anomaly (FP):\", np.sum(fp_idx))\n",
    "print(\"False Normal (FN):\", np.sum(fn_idx))\n",
    "print(\"True Anomaly (TP):\", np.sum(tp_idx))\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Retrieve Original Row Numbers for Each Category\n",
    "# -------------------------------\n",
    "# idx_test holds the original row numbers for the test set.\n",
    "tn_rows = idx_test[tn_idx]\n",
    "fp_rows = idx_test[fp_idx]\n",
    "fn_rows = idx_test[fn_idx]\n",
    "tp_rows = idx_test[tp_idx]\n",
    "\n",
    "print(\"\\nRow Numbers:\")\n",
    "print(\"True Normal (TN) row indices:\", tn_rows.tolist())\n",
    "print(\"False Anomaly (FP) row indices:\", fp_rows.tolist())\n",
    "print(\"False Normal (FN) row indices:\", fn_rows.tolist())\n",
    "print(\"True Anomaly (TP) row indices:\", tp_rows.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bot Daten in richtiges Format bringen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Pfad zum Ordner mit den JSON-Dateien\n",
    "json_folder_path = '/Users/tuhin/Desktop/Bachelorarbeit/sapiagent/feature_berechnung_ownhuman/seleniumTestdaten'\n",
    "# Pfad zum Ordner für die Ausgabe-CSV-Dateien\n",
    "csv_output_folder_path = '/Users/tuhin/Desktop/Bachelorarbeit/sapiagent/feature_berechnung_ownhuman/bot_nutzbar'\n",
    "\n",
    "# Erstellen Sie den Ausgabeordner, falls er nicht existiert\n",
    "os.makedirs(csv_output_folder_path, exist_ok=True)\n",
    "\n",
    "# Benutzerzähler initialisieren\n",
    "user_counter = 1\n",
    "\n",
    "# Alle JSON-Dateien im Ordner durchlaufen\n",
    "anzahkSkips = 0\n",
    "anzahlPC = 0\n",
    "for filename in os.listdir(json_folder_path):\n",
    "    if filename.endswith('.json'):\n",
    "        json_file_path = os.path.join(json_folder_path, filename)\n",
    "        \n",
    "        # JSON-Datei lesen\n",
    "        with open(json_file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        \n",
    "        \n",
    "        if \"Android\" in data[\"userAgent\"] or \"iPhone\" in data[\"userAgent\"]:\n",
    "            anzahkSkips += 1\n",
    "            continue\n",
    "\n",
    "        if \"Windows\" in data[\"userAgent\"] or \"Macintosh\" in data[\"userAgent\"]:\n",
    "            anzahlPC += 1\n",
    "\n",
    "\n",
    "\n",
    "        # Den ersten Zeitstempel als Startzeitpunkt festlegen\n",
    "        start_timestamp = data['mouseEvents'][0]['timestamp']\n",
    "        \n",
    "        # Benutzerordner erstellen\n",
    "        user_folder = os.path.join(csv_output_folder_path, f'user{user_counter}')\n",
    "        os.makedirs(user_folder, exist_ok=True)\n",
    "        \n",
    "        # Datum aus dem Dateinamen extrahieren\n",
    "        date_str = filename.split('_')[1]\n",
    "        date_obj = datetime.strptime(date_str, '%Y%m%d')\n",
    "        formatted_date = date_obj.strftime('%Y_%m_%d')\n",
    "        \n",
    "        # Pfad zur Ausgabe-CSV-Datei\n",
    "        csv_file_name = f'session_{formatted_date}_3min.csv'\n",
    "        csv_file_path = os.path.join(user_folder, csv_file_name)\n",
    "        \n",
    "        # CSV-Datei schreiben\n",
    "        with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            # CSV-Header schreiben\n",
    "            csv_writer.writerow(['client timestamp', 'button', 'state', 'x', 'y'])\n",
    "            \n",
    "            # MouseEvents in CSV-Format konvertieren und schreiben\n",
    "            for event in data['mouseEvents']:\n",
    "                relative_timestamp = event['timestamp'] - start_timestamp\n",
    "                button_state = 'NoButton'\n",
    "                if event['button'] == 1:\n",
    "                    button_state = 'LeftButton'\n",
    "                elif event['button'] == 2:\n",
    "                    button_state = 'RightButton'\n",
    "                \n",
    "                # Mausereignistypen ändern\n",
    "                event_type = event['type']\n",
    "                if event_type == 'mousemove':\n",
    "                    event_type = 'Move'\n",
    "                elif event_type == 'mousedown':\n",
    "                    event_type = 'Pressed'\n",
    "                elif event_type == 'mouseup':\n",
    "                    event_type = 'Released'\n",
    "                \n",
    "                csv_writer.writerow([relative_timestamp, button_state, event_type, event['x'], event['y']])\n",
    "        \n",
    "        # Benutzerzähler erhöhen\n",
    "        user_counter += 1\n",
    "\n",
    "print(f\"Anzahl der Skips: {anzahkSkips}\")\n",
    "print(f\"Anzahl der PC: {anzahlPC}\")\n",
    "print(\"Alle Mouse events wurden erfolgreich konvertiert und gespeichert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['chunk_id', 'geschwindigkeit', 'geschwindigkeit_min',\n",
       "       'geschwindigkeit_max', 'geschwindigkeit_mean', 'geschwindigkeit_var',\n",
       "       'dauer', 'direkte_distanz', 'effizienz', 'totale_distanz',\n",
       "       'smoothness_v1', 'smoothness_v2', 'beschleunigung_min',\n",
       "       'beschleunigung_max', 'beschleunigung_mean', 'beschleunigung_var',\n",
       "       'geschwindigkeit_zscore', 'geschwindigkeit_min_zscore',\n",
       "       'geschwindigkeit_max_zscore', 'geschwindigkeit_mean_zscore',\n",
       "       'geschwindigkeit_var_zscore', 'beschleunigung_min_zscore',\n",
       "       'beschleunigung_max_zscore', 'beschleunigung_mean_zscore',\n",
       "       'beschleunigung_var_zscore', 'dauer_zscore', 'direkte_distanz_zscore',\n",
       "       'effizienz_zscore', 'totale_distanz_zscore', 'smoothness_v1_zscore',\n",
       "       'smoothness_v2_zscore', 'geschwindigkeit_minmax',\n",
       "       'geschwindigkeit_min_minmax', 'geschwindigkeit_max_minmax',\n",
       "       'geschwindigkeit_mean_minmax', 'geschwindigkeit_var_minmax',\n",
       "       'beschleunigung_min_minmax', 'beschleunigung_max_minmax',\n",
       "       'beschleunigung_mean_minmax', 'beschleunigung_var_minmax',\n",
       "       'dauer_minmax', 'direkte_distanz_minmax', 'effizienz_minmax',\n",
       "       'totale_distanz_minmax', 'smoothness_v1_minmax', 'smoothness_v2_minmax',\n",
       "       'is_anomaly'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Index(['chunk_id', 'geschwindigkeit', 'geschwindigkeit_min',\n",
    "       'geschwindigkeit_max', 'geschwindigkeit_mean', 'geschwindigkeit_var',\n",
    "       'dauer', 'direkte_distanz', 'effizienz', 'totale_distanz',\n",
    "       'smoothness_v1', 'smoothness_v2', 'beschleunigung_min',\n",
    "       'beschleunigung_max', 'beschleunigung_mean', 'beschleunigung_var',\n",
    "       'geschwindigkeit_zscore', 'geschwindigkeit_min_zscore',\n",
    "       'geschwindigkeit_max_zscore', 'geschwindigkeit_mean_zscore',\n",
    "       'geschwindigkeit_var_zscore', 'beschleunigung_min_zscore',\n",
    "       'beschleunigung_max_zscore', 'beschleunigung_mean_zscore',\n",
    "       'beschleunigung_var_zscore', 'dauer_zscore', 'direkte_distanz_zscore',\n",
    "       'effizienz_zscore', 'totale_distanz_zscore', 'smoothness_v1_zscore',\n",
    "       'smoothness_v2_zscore', 'geschwindigkeit_minmax',\n",
    "       'geschwindigkeit_min_minmax', 'geschwindigkeit_max_minmax',\n",
    "       'geschwindigkeit_mean_minmax', 'geschwindigkeit_var_minmax',\n",
    "       'beschleunigung_min_minmax', 'beschleunigung_max_minmax',\n",
    "       'beschleunigung_mean_minmax', 'beschleunigung_var_minmax',\n",
    "       'dauer_minmax', 'direkte_distanz_minmax', 'effizienz_minmax',\n",
    "       'totale_distanz_minmax', 'smoothness_v1_minmax', 'smoothness_v2_minmax',\n",
    "       'is_anomaly'],\n",
    "      dtype='object')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sapivenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
